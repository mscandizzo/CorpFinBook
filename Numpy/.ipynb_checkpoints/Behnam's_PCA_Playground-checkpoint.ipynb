{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Behnam's PCA Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW2OkwNkF16a",
        "colab_type": "text"
      },
      "source": [
        "# Principle Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga4ghBizelPt",
        "colab_type": "text"
      },
      "source": [
        "## PCA Introduction: Why Reduce?\n",
        "\n",
        "Principle Component Analysis, or PCA, is just one of several tools in an data scientist's toolkit to service the same problem: reducing the dimensionality of a data set. To truly understand the motivation behind the various steps in PCA, it is worth questioning why we want to reduce the dimensionality of our dataset in the first place. This is a bit of rehashing, so if you already feel confident about this feel free to skip to the next section. \n",
        "\n",
        "**What is high dimensionality?** \n",
        "\n",
        "A $p$-dimensional data set is one where each data point has been characterized across $p$ different dimensions: If you have data about people, you might know a given person's age, height, weight, gender, income, and years of formal education. Since we've characterized this person across 6 different dimensions, we could represent them by a single point in an 6-dimensional space. Similarly, a data set of countries that contained a given country's population, landmass, and GDP can be represented in a 3-dimensional space. \n",
        "\n",
        "So how might we know when a data set has *high* dimensionality? There are a few different definitions floating around, but the one I like comes from the context of regression and classification algorithms (you will see regression next week, and some classification algorithms during the Unsupervised Learning unit) and points toward some of the inherent issues with a high dimensional data set (defined as such): for a dataset with $n$ many datapoints and $p$ dimensions, we would consider the data as high-dimensional when $p > n$, that is, when there are more dimensions than there are datapoints. This is a rough heuristic, but it illustrates some obvious challenges associated with high dimensionality (if lengthy academic papers are your jam, [this one](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865881/) gives extensive treatment to the $p>n$ problem). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDtAChb6p765",
        "colab_type": "text"
      },
      "source": [
        "**Why it's a problem:** \n",
        "\n",
        "With linear regression, the implications of $p>n$ are easily illustrated. Let's look at a trivial example: suppose we want to find a \"best fit line\" (that's all a regression line is) for a high-dimenional data set, particularly where $p=2$ and $n=1$. In other words, we have a single data point characterized across two dimensions. Our data looks something like (I warned you it would be trivial):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irHIQlbWkDVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "b3813217-322c-4bc5-cf73-ad278cb2b9bd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter([1],[1],color='r',s=3000)\n",
        "plt.xlim(0, 2)\n",
        "plt.ylim(0, 2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"\\'' Data ''\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAFnCAYAAACoxECQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3466d332e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtX97Vsl2Uy",
        "colab_type": "text"
      },
      "source": [
        "And the best fit line something like:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBjlSQefS96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "639329de-8476-42f2-9fea-ccce5876da42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, sharey=True)\n",
        "\n",
        "\n",
        "ax1.scatter([1],[1],color='r')\n",
        "ax1.plot([0,1,2])\n",
        "\n",
        "ax2.scatter([1],[1],color='r')\n",
        "ax2.plot([-1,1,3])\n",
        "\n",
        "ax3.scatter([1],[1],color='r')\n",
        "ax3.plot([4,1,-2])\n",
        "\n",
        "ax4.scatter([1],[1],color='r')\n",
        "ax4.plot([-4,1,6])\n",
        "\n",
        "ax5.scatter([1],[1],color='r')\n",
        "ax5.plot([(1-.1),1,(1+.1)])\n",
        "\n",
        "f.subplots_adjust(hspace=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34670d8cf8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGBP6st0mLIT",
        "colab_type": "text"
      },
      "source": [
        "Clearly, trying to find the \"best-fit line\"  in this scenario is an exercise in nonsense. No two of the lines above are the same, yet each perfectly \"fits\" the data. When $p>n$ regression analysis isn't well-defined, as there are infinitely many, equally \"best fiting\" lines. For this reason, performing regression on high-dimensional data set is generally a bad idea, as it might not give you meaningful results.\n",
        "\n",
        "For classification algorithms, $p>n$ is also bad news: high-dimensional data are sparse data, and sparse data can pose several problems to classification algorithms. The K-Nearest Neighbors algorithm, for example, classifies data by training off of nearby points. The problem arises because [\"nearness\"](https://www.edupristine.com/blog/curse-dimensionality) gets diluted as dimensions increase. A simple [categorical classifier](http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/) encounters similar issues.\n",
        "\n",
        "Hopefully this illustrates a bit of the motivation behind reducing the number of dimensions in a data set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTd9djVfTDG",
        "colab_type": "text"
      },
      "source": [
        "**How to reduce:**\n",
        "\n",
        "If fewer dimensions were an unequivocally intrinsic good, we could just toss out some arbitrarily selected dimensions. Obviously, this is ill-advised, as you might toss meaningful information out with the discarded dimensions. This clarifies the purpose of any effective dimensionality reduction technique: an ideal approach would find an alternative representation of the data in fewer dimensions that still preserves as much of the information contained in the data as possible.\n",
        "\n",
        "This is PCA. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIy_zEVW4WJ5",
        "colab_type": "text"
      },
      "source": [
        "## PCA Step 1: Constructing The Covariance Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLPh2TZE527W",
        "colab_type": "text"
      },
      "source": [
        "If you ask a statistician about PCA, they might say something like \n",
        "\n",
        "> *\"PCA identifies the principle components of a set of data by computing the eigenvectors of the covariance matrix (or sometimes correlation matrix). The eigenvector with the largest eigenvalue corresponds to the the first principle component, the eigenvector with the second largest eigenvalue to the second principle component, and so on. A specified number of principle components are used as bases for a subspace onto which the data are projected.\" (Arzaghi, 2018)*\n",
        "\n",
        "While this isn't wrong, if \"covariance matrix\" and \"eigenvectors\" aren't part of your vernacular, it's a bit meaningless. We move forward with an example to illustrate what these terms mean, why they are important to PCA, and how we compute them. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHKoVMjKIqLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import all the stuff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "py.sign_in('Behnamanam', '0JQM7oVBKe1uWVl47eVu')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQb8cmW1ItlP",
        "colab_type": "text"
      },
      "source": [
        "Suppose we had data for the following nine people, where we know each individual's height, weight, and age:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUe8FeM0JASn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "People = {\n",
        "    'Height in Meters: ': [1.7, 1.8, 1.65, 1.75, 1.9, 1.77, 1.74, 1.82, 1.67], \n",
        "    'Weight in Kg: ': [82, 81, 68, 76, 87, 77, 76, 83, 59], \n",
        "    'Age in Years: ': [22, 41, 28, 36, 47, 51, 23, 34, 64]}\n",
        "\n",
        "ExampleOne = pd.DataFrame(data=People)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1xufvabJAxd",
        "colab_type": "text"
      },
      "source": [
        "While this isn't an ideal candidate for a real-life implementation of PCA ( $p=3$ and $n = 9$, so $p \\ngtr n$ in this case), its 3 dimensions are easy to visualize in a 3 dimensional space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTU5x_pzTDJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "91e8597d-91e7-407f-c390-d4b0b788a3e3"
      },
      "source": [
        "x = ExampleOne['Height in Meters: ']\n",
        "y = ExampleOne['Weight in Kg: ']\n",
        "z = ExampleOne['Age in Years: ']\n",
        "\n",
        "traceMetric = go.Scatter3d(\n",
        "    x=(x - x.mean()),\n",
        "    y=(y - y.mean()),\n",
        "    z=(z - z.mean()),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color=[\"blue\" for x in range(0,len(x))],\n",
        "        colorscale = 'Viridis',\n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Meters'\n",
        ")\n",
        "\n",
        "data = [traceMetric]\n",
        "\n",
        "layout = go.Layout(scene = dict(\n",
        "                    xaxis = dict(\n",
        "                        title='Height in Meters'),\n",
        "                    yaxis = dict(\n",
        "                        title='Weight in KG'),\n",
        "                    zaxis = dict(\n",
        "                        title='Age in Years'),),\n",
        "                    width=700,\n",
        "                    margin=dict(\n",
        "                    r=20, b=10,\n",
        "                    l=10, t=10)\n",
        "                  )\n",
        "\n",
        "fig = go.Figure(data=[traceMetric], layout=layout)\n",
        "py.iplot(fig, filename='3d-scatter-colorscale0')\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Behnamanam/18.embed\" height=\"525px\" width=\"700px\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9y1V6pKihih",
        "colab_type": "text"
      },
      "source": [
        "Move around the plot above enough and you might find that, at the right angle, the points line up, indicating that the data occupy roughly the same plane, or two-dimensional subspace. Of course, this is by design because I made this example up: in a 4, 5, or 200 dimensional space this kind of visual inspection is no longer feasible, which is why dimensionality reduction techniques are useful. Either way, we're going to use PCA to take this data from three dimensions to two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8riyB8UAnZxr",
        "colab_type": "text"
      },
      "source": [
        "### PCA steps 0.1 & 0.2: centering the data, standardizing the data (i.e. \"What Is Covariance?\")\n",
        "\n",
        "There are two things you will hear people consider when constructing a covariance matrix for PCA: whether to center the data, and whether to standardize the data. Why we would want to do either is better understood after we have a good idea of what covariance is. As you might suspect, the mathematical definition of covariance is very similar to the definition for variance. A quick refresher on the mathematical definition of variance:\n",
        "\n",
        "$\\qquad V = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\bar{x})^2 \\quad$ or, alternatively: $\\quad\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\mu)^2$\n",
        "\n",
        "Where the two equations are identical, but the second one has more greek letters to signal intellectual superiority (to be fair, lower case sigma \"$\\sigma$\" is also the canonical greek letter for standard deviation, so saying the definition of variance is equal to $\\sigma^2$ is actually a nice way of demonstrating that the standard deviation is directly related to, and found by taking the square root of, the variance). Variance gives a quantitative description of how much \"variety\" exist the values in $x$ by measuring how far each value $x_i$ is from the average (in the first equation given by $\\bar{x}$, in the second by the greek letter mu \"$\\mu$\"), squaring it (this is just one way of getting a distance from a difference. [People argue](http://www.leeds.ac.uk/educol/documents/00003759.htm) about the best way to do this), and adding all these squared distances up then dividing by the number of values $n$. Intuitively, a small variance suggests that all $x$ values hover near the mean value of $x$, while a large variance suggests that $x$ values are all over the place. \n",
        "\n",
        "Now to covariance. Mathematically, covariance looks similar to variance, but instead of an equation with a single variable $x$, covariance describes a relationship between two variables $x$ and $y$: \n",
        "\n",
        "$\\qquad COV(x,y) = \\frac{1}{n-1}\\sum\\limits_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})$\n",
        "\n",
        "We could go through an exhaustive comparison of the equations for variance and covariance to reveal how intepretations of the two diverge... or I'll leave that as an \"exercise for a reader\" and point out the main takeaways: *covariance* describes the extent to which $x$ and $y$ exhibit a linear relationship. A large positive covariance indicates that there is a positive linear relationship (as $x$ increases, so does $y$), while a large negative covariance indicates a negative linear relationship. A covariance of zero implies no linear relationship. \n",
        "\n",
        "\n",
        "Hidden within the mathematical definition of covariance rests the crux of the discussions both about centering and standardizing: \n",
        "\n",
        "**0.1 - Centering** by itself isn't necessary to construct the desired covariance matrix: to center a data set, one simply subtracts the mean from each data point. \n",
        "\n",
        "$\\qquad x_{centered} = {x-\\bar{x}}$\n",
        "\n",
        "This is exactly what is occuring inside the summation when you calculate covariance (and variance, for that matter). So, *if* you construct covariance matrix directly (yep, there are other ways that we won't worry about), centering the data is redundant, because you'll arrive at the same covariance matrix either way.\n",
        "\n",
        "**0.2 - Standardizing**, on the other hand, is another problem entirely. Standardizing is given by:\n",
        "\n",
        "$\\qquad x' = \\frac{x-\\bar{x}}{\\sigma}$\n",
        "\n",
        "And is used to ensure that the units of measurement don't end up distorting your covariance matrix: if you look at the definition of covariance, the units of covariance between some variables $x$ and $y$ end up being the product of the units of those two variables. This should strike you as more than a little silly. In our example, a description of the covariance between the height and weight would be given in \"kilogram meters\"  (whatever those are). Of course, the bigger issue is not that these units sound silly, but that one can just as easily have measured height in inches or millimeters or potatoes, each of which would result in a *different covariance*. To build a visual intuition, consider the graph below, where we plot the same data as above, but where height is given in meters, inches, and millimeters:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x3T5_ZjHRDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "a018fcef-e332-4c7c-8ed6-b173c82e8ea9"
      },
      "source": [
        "xinches = ExampleOne['Height in Meters: ']*39.37\n",
        "\n",
        "traceMetric = go.Scatter3d(\n",
        "    x=(x - x.mean()),\n",
        "    y=(y - y.mean()),\n",
        "    z=(z - z.mean()),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color=[\"blue\" for x in range(0,len(x))],\n",
        "        colorscale = 'Viridis',\n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Meters'\n",
        ")\n",
        "\n",
        "\n",
        "traceImperial = go.Scatter3d(\n",
        "    x=(xinches - xinches.mean()),\n",
        "    y=(y - y.mean()),\n",
        "    z=(z - z.mean()),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color= [\"magenta\" for x in range(0,len(x))], # set color to an array/list of desired values\n",
        "        colorscale = 'Jet',   # choose a colorscale\n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Inches'\n",
        ")\n",
        "\n",
        "xmilli = ExampleOne['Height in Meters: ']*1000.0\n",
        "\n",
        "traceMilli = go.Scatter3d(\n",
        "    x=(xmilli - xmilli.mean()),\n",
        "    y=(y - y.mean()),\n",
        "    z=(z - z.mean()),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color= [\"teal\" for x in range(0,len(x))], # set color to an array/list of desired values\n",
        "        colorscale = 'Jet',   # choose a colorscale\n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Millimeters'\n",
        ")\n",
        "\n",
        "data = [traceMetric,traceImperial,traceMilli]\n",
        "layout = go.Layout(scene = dict(\n",
        "                    xaxis = dict(\n",
        "                        title='Height in arbitrary units'),\n",
        "                    yaxis = dict(\n",
        "                        title='Weight in KG', range=[-15*100, 10*100]),\n",
        "                    zaxis = dict(\n",
        "                        title='Age in Years', range=[-15*100, 25*100])),\n",
        "                    \n",
        "                    margin=dict(\n",
        "                    r=20, b=10,\n",
        "                    l=10, t=10)\n",
        "                  )\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename='3d-scatter-colorscale2')\n",
        "\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Behnamanam/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx0qPcwUPilv",
        "colab_type": "text"
      },
      "source": [
        "The result is a \"stretching\", if you will, of the height axis until becomes the dominant characteristic of the data set.\n",
        "\n",
        "To be fair, I've done two things to the graph above which aren't meant to be sneaky but do warrant explanation: first, I set the range of the \"Weight\" and Years\" axes to be roughly what we would observe if we had to \"zoom out\" to capture the heights given in their different units. Second, I centered the data for legibility, (but this also helps drive home that centering your data alone won't protect it from unit distortion). Both of these are in service of the overall point: the plot above shows that changing the units can vastly distort the apparent shape of the data. For height in millimeters, visual inspection might lead us to conclude that almost all of the data can be explained by this dimension of height alone.\n",
        "\n",
        "Thankfully, we can avoid this issue entirely by standardizing our units which, yes, also centers the data. Much more importantly, standardizing expresses all $x$ values in terms of $x$'s' standard deviation (this is sometimes called scaling to *unit variance*). Observe what happens when we standardize and replot the graph above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_pBMEkCTkNS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "aa02f428-cc7a-4830-f81c-2cb63b60ce6a"
      },
      "source": [
        "\n",
        "traceMetric = go.Scatter3d(\n",
        "    x=(x - x.mean())/np.std(x),\n",
        "    y=(y - y.mean())/np.std(y),\n",
        "    z=(z - z.mean())/np.std(z),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color=[\"blue\" for x in range(0,len(x))],\n",
        "        colorscale = 'Viridis',\n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Meters'\n",
        ")\n",
        "\n",
        "traceImperial = go.Scatter3d(\n",
        "    x=(xinches - xinches.mean())/np.std(xinches),\n",
        "    y=(y - y.mean())/np.std(y),\n",
        "    z=(z - z.mean())/np.std(z),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color= [\"magenta\" for x in range(0,len(x))], \n",
        "        colorscale = 'Jet',   \n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Inches'\n",
        ")\n",
        "\n",
        "traceMilli = go.Scatter3d(\n",
        "    x=(xmilli - xmilli.mean())/np.std(xmilli),\n",
        "    y=(y - y.mean())/np.std(y),\n",
        "    z=(z - z.mean())/np.std(z),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color= [\"teal\" for x in range(0,len(x))], \n",
        "        colorscale = 'Jet',  \n",
        "        opacity=0.8\n",
        "    ),\n",
        "    name='Height in Millimeters'\n",
        ")\n",
        "\n",
        "data = [traceMetric,traceImperial,traceMilli]\n",
        "layout = go.Layout(scene = dict(\n",
        "                    xaxis = dict(\n",
        "                        title='Standardized Height'),\n",
        "                    yaxis = dict(\n",
        "                        title='Standardized Weight',),\n",
        "                    zaxis = dict(\n",
        "                        title='Standardized Age', )),\n",
        "                    \n",
        "                    margin=dict(\n",
        "                    r=20, b=10,\n",
        "                    l=10, t=10)\n",
        "                  )\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename='3d-scatter-colorscale3')\n",
        "\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Behnamanam/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJYlMIl7VJBm",
        "colab_type": "text"
      },
      "source": [
        "We have total agreement irrespective the units in which the data is given! What a relief. With very few exceptions, it is good practice to standardize your data before performing PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEO8-rT-WSHE",
        "colab_type": "text"
      },
      "source": [
        "### PCA steps 1.0 & 1.1: Constructing the Covariance and Correlation Matrices\n",
        "\n",
        "**1.0 - Covariance Matrix:** \n",
        "\n",
        "Now that we know what covariance is, we can construct a covariance matrix (and then argue for constructing the correlation matrix instead). For a dataset with $n$ variables (or dimensions, I've kind of been using the two interchangeably...) given by $x_1, x_2, \\dots x_n$, the covariance matrix is constructed by calculating the covariance across all pairwise combinations:\n",
        "\n",
        "\\begin{pmatrix}\n",
        "        COV(x_1,x_1) & COV(x_1,x_2) & \\cdots & COV(x_1,x_n) \\\\ \n",
        "        COV(x_2,x_1) & COV(x_2,x_2) &  &  \\\\\n",
        "        \\vdots &  & \\ddots &  \\\\\n",
        "        COV(x_n,x_1) &  &  & COV(x_n,x_n) \\\\\n",
        "    \\end{pmatrix}\n",
        "    \n",
        "This matrix has a lot of really nice properties like being square, having symmetry across the diagonal, and having positive (and in most cases, non-zero) values along the diagonal. People who care a lot about matrices call this type of matrix \"positive semi-definite\". For the inquisitive reader, here's [an explanation](http://www.utdallas.edu/~herve/Abdi-EVD2007-pretty.pdf) about how this property relates to PCA, but the skinny is that positive semi-definite matrices have the eigenvectors we're looking for. \n",
        "\n",
        "**1.1 - Correlation Matrix**\n",
        "\n",
        "In section 0.2 I argued for the benefits of always standardizing your data before calculating covariance. Doing this leads us (not so coincidently) right to the mathematical definition of the very close cousin of covariance, the correlation coefficient:\n",
        "\n",
        "$\\qquad \\rho_{x,y} = \\frac{COV(x,y)}{\\sigma_x\\sigma_y}$\n",
        "\n",
        "Denoted by the Greek letter rho \"$\\rho$\", correlation is sometimes called \"Pearson's r\", or \"Pearson's correlation coefficient\". But no matter what you call it, correlation is essentially the standardized version of covariance. This is evident by the terms in the denominator, the standard deviation of $x$ multiplied by the standard deviation of $y$. While it has the obvious benefit of being unit-invariant (because it's standardized!), correlation also has the pleasant property of being equal to $1$ when two variables completely covary (i.e. when they fit a straight line with a positive slope), and $-1$ when two variables completely covary inversely. \n",
        "\n",
        "The implication of this is that standardizing your data before constructing the covariance matrix means you are actually constructing a correlation matrix (in the representation below, I've taken the liberty of simplifying all the entries along the diagonal to 1, as all variables correlate with themselves):  \n",
        "\n",
        "\\begin{pmatrix}\n",
        "        1 & \\rho_{x_1,x_2} & \\cdots & \\rho_{x_1,x_n} \\\\ \n",
        "        \\rho_{x_2,x_1} & 1 &  &  \\\\\n",
        "        \\vdots &  & \\ddots &  \\\\\n",
        "        \\rho_{x_n,x_1} &  &  & 1 \\\\\n",
        "    \\end{pmatrix}\n",
        "\n",
        "If its standardized nature doesn't convince you to prefer the correlation matrix, don't take my word for it. Take Ian T. Jolliffe's, who wrote [a 488 page tome](http://cda.psych.uiuc.edu/statistical_learning_course/Jolliffe%20I.%20Principal%20Component%20Analysis%20%282ed.,%20Springer,%202002%29%28518s%29_MVsa_.pdf) on PCA: \n",
        "\n",
        ">*\"A major argument for using correlation—rather than covariance—\n",
        "matrices to define PCs is that the results of analyses for different sets\n",
        "of random variables are more directly comparable than for analyses based\n",
        "on covariance matrices. The big drawback of PCA based on covariance matrices\n",
        "is the sensitivity of the PCs to the units of measurement used for\n",
        "each element of x...* \n",
        "\n",
        ">*...It is unwise to use PCs on a covariance\n",
        "matrix when x consists of measurements of different types, unless there is a\n",
        "strong conviction that the units of measurements chosen for each element of\n",
        "x are the only ones that make sense. Even when this condition holds, using\n",
        "the covariance matrix will not provide very informative PCs if the variables\n",
        "have widely differing variances. Furthermore, with covariance matrices and\n",
        "non-commensurable variables the PC scores are difficult to interpret—what\n",
        "does it mean to add a temperature to a weight? For correlation matrices, the\n",
        "standardized variates are all dimensionless and can be happily combined\n",
        "to give PC scores (Legendre and Legendre, 1983, p. 129).\" (Jolliffe, I. 2002)*\n",
        "\n",
        "That's straight from the horse's mouth, folks: be wise, standardize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7OTW1p0Piq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "2d97f620-eb61-4287-9cf2-68cc7248102b"
      },
      "source": [
        "#print(ExampleOne.std())\n",
        "#\n",
        "#\n",
        "#print(ExampleOne.std())\n",
        "#\n",
        "#print(ExampleOne.std())\n",
        "#new_column = pd.Series(xinches, name='Height in Inches', index=[0, 2])\n",
        "\n",
        "#Imperial = {'Height in Meters: ': [h*39.7 for h in x], \n",
        "#     'Weight in Kg: ': [82, 81, 68, 76, 87, 77, 76, 83, 59], \n",
        "#     'Age in Years: ': [22, 41, 28, 36, 47, 51, 23, 34, 64]}\n",
        "#ExampleTwo = pd.DataFrame(data=Imperial)\n",
        "#print(ExampleTwo.std())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(np.cov([x,y,z]).round(decimals=3))\n",
        "\n",
        "print(np.cov([(x - x.mean()), (y - y.mean()), (z - z.mean())]))\n",
        "    \n",
        "print(np.cov([xinches,y,z]).round(decimals=3))\n",
        "print(np.cov([xmilli,y,z]).round(decimals=3))\n",
        "#\n",
        "print(np.corrcoef([x,y,z]).round(decimals=3))\n",
        "print(np.corrcoef([xinches,y,z]).round(decimals=3))\n",
        "print(np.corrcoef([xmilli,y,z]).round(decimals=3))\n",
        "print(np.corrcoef(xmilli,[y,z]).round(decimals=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6.00000e-03  5.30000e-01  1.66000e-01]\n",
            " [ 5.30000e-01  7.27780e+01 -4.51530e+01]\n",
            " [ 1.66000e-01 -4.51530e+01  1.91778e+02]]\n",
            "[[ 6.12777778e-03  5.30277778e-01  1.65972222e-01]\n",
            " [ 5.30277778e-01  7.27777778e+01 -4.51527778e+01]\n",
            " [ 1.65972222e-01 -4.51527778e+01  1.91777778e+02]]\n",
            "[[  9.498  20.877   6.534]\n",
            " [ 20.877  72.778 -45.153]\n",
            " [  6.534 -45.153 191.778]]\n",
            "[[6127.778  530.278  165.972]\n",
            " [ 530.278   72.778  -45.153]\n",
            " [ 165.972  -45.153  191.778]]\n",
            "[[ 1.     0.794  0.153]\n",
            " [ 0.794  1.    -0.382]\n",
            " [ 0.153 -0.382  1.   ]]\n",
            "[[ 1.     0.794  0.153]\n",
            " [ 0.794  1.    -0.382]\n",
            " [ 0.153 -0.382  1.   ]]\n",
            "[[ 1.     0.794  0.153]\n",
            " [ 0.794  1.    -0.382]\n",
            " [ 0.153 -0.382  1.   ]]\n",
            "[[ 1.     0.794  0.153]\n",
            " [ 0.794  1.    -0.382]\n",
            " [ 0.153 -0.382  1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}